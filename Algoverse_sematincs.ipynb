{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPlWJ3h4lD5ctMpHDW7rkRU"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "My First Sparse Autoencoder\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "import numpy\n",
        "\n",
        "\n",
        "from transformers import GPT2Tokenizer, GPT2Model\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "# For progress bar\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# === SETTINGS ===\n",
        "model_name = \"stanford-crfm/caprica-gpt2-small-x81\"\n",
        "layer_index = 6     # 6 is middle layer\n",
        "input_size = 3072\n",
        "hidden_size = 8192\n",
        "batch_size_num = 32 # Might need to reduce if crashes\n",
        "\n",
        "\n",
        "# ********** PART 1: LOAD THE MODEL **********\n",
        "print(\"Loading model...\")\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load actual model\n",
        "gpt_model = GPT2Model.from_pretrained(model_name)\n",
        "gpt_model.eval()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpt_model.cuda()\n",
        "    print(\"Using GPU!\")\n",
        "else:\n",
        "    print(\"Using CPU :(\")\n",
        "\n",
        "\n",
        "# ~~~~~ Hook to get activations ~~~~~\n",
        "activations_storage = {}  # Dictionary to store values\n",
        "\n",
        "def save_activations(name):\n",
        "    def hook(model, input, output):\n",
        "        activations_storage[name] = output[0].detach()\n",
        "    return hook\n",
        "\n",
        "gpt_model.h[layer_index].mlp.c_fc.register_forward_hook(\n",
        "    save_activations('middle_layer')\n",
        ")\n",
        "\n",
        "\n",
        "# ++++++++++ AUTOENCODER CLASS ++++++++++\n",
        "class MyAE(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        # Encoder part\n",
        "        self.encoder_layer = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU() # Non-linear activation function\n",
        "\n",
        "        # Decoder part\n",
        "        self.decoder_layer = nn.Linear(hidden_dim, input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Step 1: Encode\n",
        "        hidden = self.relu(self.encoder_layer(x))\n",
        "        # Step 2: Decode\n",
        "        output = self.decoder_layer(hidden)\n",
        "        return output, hidden\n",
        "\n",
        "# Make autoencoder\n",
        "my_autoencoder = MyAE(input_size, hidden_size)\n",
        "if torch.cuda.is_available():\n",
        "    my_autoencoder.cuda()\n",
        "\n",
        "\n",
        "print(\"\\nPreparing dataset...\")\n",
        "\n",
        "\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
        "\n",
        "def tokenize_data(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        max_length=128,  # Fixed number from tutorial\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "\n",
        "dataset = dataset.map(tokenize_data, batched=True)\n",
        "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=batch_size_num,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "print(\"\\nStarting training...\")\n",
        "\n",
        "optimizer = Adam(my_autoencoder.parameters(), lr=0.0001)\n",
        "loss_function = nn.MSELoss()\n",
        "\n",
        "# Store losses\n",
        "losses = []\n",
        "\n",
        "# Loop for epochs\n",
        "for epoch in range(10):  # Find better number for epoch\n",
        "    print(f\"\\nEpoch {epoch+1}/10\")\n",
        "\n",
        "    total_loss = 0.0\n",
        "    avg_activation = 0.0\n",
        "\n",
        "    # Progress bar\n",
        "    progress_bar = tqdm(train_loader, desc=\"Processing batches\")\n",
        "\n",
        "    for batch in progress_bar:\n",
        "        input_ids = batch[\"input_ids\"].cuda() if torch.cuda.is_available() else batch[\"input_ids\"]\n",
        "        attention_mask = batch[\"attention_mask\"].cuda() if torch.cuda.is_available() else batch[\"attention_mask\"]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = gpt_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        acts = activations_storage['middle_layer']\n",
        "        batch_size, seq_len, feat_dim = acts.shape\n",
        "        flattened_acts = acts.view(-1, feat_dim)\n",
        "\n",
        "        # Autoencoder step\n",
        "        reconstructed, hidden = my_autoencoder(flattened_acts)\n",
        "\n",
        "        # Calculate loss\n",
        "        reconstruction_loss = loss_function(reconstructed, flattened_acts)\n",
        "        sparsity_loss = 0.01 * torch.mean(torch.abs(hidden))  #Check with universality paper\n",
        "\n",
        "        total_loss = reconstruction_loss + sparsity_loss\n",
        "\n",
        "        # Backprop\n",
        "        optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        losses.append(total_loss.item())\n",
        "        progress_bar.set_postfix(loss=total_loss.item())\n",
        "    avg_loss = sum(losses[-len(train_loader):])/len(train_loader)\n",
        "    print(f\"Epoch {epoch+1} - Loss: {avg_loss:.4f}\")\n",
        "\n",
        "torch.save(my_autoencoder.state_dict(), \"my_sparse_ae.pth\")\n",
        "print(\"\\nTraining complete! Saved model.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9ajL55AIqJg",
        "outputId": "4c334904-5a3b-4a19-d4e4-6e535ddecb99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 1148/1148 [05:24<00:00,  3.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Loss: 0.0953, Avg Activation: 0.2030\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 1148/1148 [05:24<00:00,  3.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 - Loss: 0.0482, Avg Activation: 0.1678\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 1148/1148 [05:24<00:00,  3.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 - Loss: 0.0334, Avg Activation: 0.1422\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 1148/1148 [05:24<00:00,  3.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 - Loss: 0.0272, Avg Activation: 0.1274\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 1148/1148 [05:23<00:00,  3.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 - Loss: 0.0232, Avg Activation: 0.1125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 1148/1148 [05:24<00:00,  3.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 - Loss: 0.0205, Avg Activation: 0.1066\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 1148/1148 [05:23<00:00,  3.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 - Loss: 0.0184, Avg Activation: 0.1016\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 1148/1148 [05:23<00:00,  3.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 - Loss: 0.0169, Avg Activation: 0.0980\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 1148/1148 [05:24<00:00,  3.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 - Loss: 0.0158, Avg Activation: 0.0939\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 1148/1148 [05:24<00:00,  3.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 - Loss: 0.0151, Avg Activation: 0.0926\n"
          ]
        }
      ]
    }
  ]
}